{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba473667-923f-408e-8b7a-8bee70200173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# =============================================================================\n",
    "# interac TRANSACTIONS CDC FEED PROJECT - REAL-TIME STREAMING PIPELINE\n",
    "# =============================================================================\n",
    "# This notebook implements a real-time streaming pipeline for interac transaction processing\n",
    "# Purpose: Processes interac transactions in real-time using CDC and structured streaming\n",
    "# Features: CDC-aware processing, merchant aggregations, error handling, and monitoring\n",
    "# Output: Real-time merchant performance metrics and transaction analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eecbb35-3530-4877-81d0-ac1b07622a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and configuration set\n",
      "Target tables: `gds_de_bootcamp_new`.default.raw_upi_transactions_v1, `gds_de_bootcamp_new`.default.merchant_aggregations\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LIBRARY IMPORTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "# Import required libraries for real-time streaming and CDC processing\n",
    "# Purpose: Set up environment for structured streaming, Delta operations, and data processing\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "# Configure Unity Catalog and table names for streaming pipeline\n",
    "# catalog_name: Unity Catalog for data governance\n",
    "# schema_name: Target schema for interac transaction tables\n",
    "# raw_table: Source table with CDC enabled for streaming\n",
    "# merchant_agg_table: Target table for merchant aggregations\n",
    "\n",
    "catalog_name = \"`interac_transaction_cdc`\"\n",
    "schema_name = \"default\"\n",
    "\n",
    "# Table names\n",
    "raw_table = f\"{catalog_name}.{schema_name}.raw_interac_transactions_v1\"\n",
    "merchant_agg_table = f\"{catalog_name}.{schema_name}.merchant_aggregations\"\n",
    "\n",
    "print(\"Libraries imported and configuration set\")\n",
    "print(f\"Target tables: {raw_table}, {merchant_agg_table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32d552e-4ea8-4f13-b5b2-b34cfc1cdb66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple merge function defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DELTA TABLE MERGE FUNCTION\n",
    "# =============================================================================\n",
    "# Implements idempotent merge operation for merchant aggregations\n",
    "# Purpose: Ensures data consistency and handles upsert operations efficiently\n",
    "# Features: Uses Delta Lake merge for ACID transactions and conflict resolution\n",
    "\n",
    "def merge_to_delta_table(delta_table_name: str, batch_df):\n",
    "    \"\"\"\n",
    "    Idempotent merge function for Delta tables\n",
    "    \n",
    "    Args:\n",
    "        delta_table_name: Name of the target Delta table\n",
    "        batch_df: DataFrame containing data to merge\n",
    "    \n",
    "    Business Logic:\n",
    "        - Merges on merchant_id, aggregation_date, and aggregation_hour\n",
    "        - Updates existing records with new values\n",
    "        - Inserts new records for new merchant/date/hour combinations\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forName(spark, delta_table_name)\n",
    "    \n",
    "    # Perform merge operation with composite key\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        batch_df.alias(\"source\"),\n",
    "        \"target.merchant_id = source.merchant_id AND target.aggregation_date = source.aggregation_date AND target.aggregation_hour = source.aggregation_hour\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "print(\"Delta table merge function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6602ef-417b-4842-9c9e-1580ad2c253f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant aggregation processing function defined\n"
     ]
    }
   ],
   "source": [
    "# Simple merchant aggregation function with CDC-aware logic\n",
    "def process_merchant_aggregations(batch_df, batch_id):\n",
    "    \"\"\"Process merchant aggregations using CDC-aware approach\"\"\"\n",
    "    try:\n",
    "        print(f\"Processing batch {batch_id} with {batch_df.count()} records\")\n",
    "        \n",
    "        # Only consider records with required fields\n",
    "        filtered_df = batch_df.filter(\n",
    "            (F.col(\"transaction_id\").isNotNull()) &\n",
    "            (F.col(\"merchant_id\").isNotNull()) &\n",
    "            (F.col(\"transaction_amount\").isNotNull()) &\n",
    "            (F.col(\"transaction_amount\") > 0) &\n",
    "            (F.col(\"transaction_timestamp\").isNotNull()) &\n",
    "            (F.col(\"transaction_status\").isNotNull()) &\n",
    "            (F.col(\"_change_type\").isin(\"insert\", \"delete\"))\n",
    "        )\n",
    "        \n",
    "        if filtered_df.count() == 0:\n",
    "            print(\"No valid records found in batch\")\n",
    "            return\n",
    "\n",
    "        # Assign +1 for insert, -1 for delete for counting\n",
    "        cdc_df = filtered_df.withColumn(\n",
    "            \"cdc_multiplier\",\n",
    "            F.when(F.col(\"_change_type\").isin(\"insert\"), F.lit(1))\n",
    "             .when(F.col(\"_change_type\").isin(\"delete\"), F.lit(-1))\n",
    "             .otherwise(F.lit(0))\n",
    "        )\n",
    "\n",
    "        # For delete, we want to \"remove\" the record\n",
    "        # For insert, we want to \"add\" the record\n",
    "\n",
    "        # Prepare aggregation columns with CDC logic\n",
    "        merchant_aggregations = cdc_df.groupBy(\n",
    "            F.col(\"merchant_id\"),\n",
    "            F.col(\"merchant_name\"),\n",
    "            F.col(\"merchant_category\"),\n",
    "            F.date_trunc(\"hour\", F.col(\"transaction_timestamp\")).cast(\"timestamp\").alias(\"aggregation_hour\"),\n",
    "            F.to_date(F.col(\"transaction_timestamp\")).alias(\"aggregation_date\")\n",
    "        ).agg(\n",
    "            (F.sum(F.col(\"cdc_multiplier\"))).alias(\"total_transactions\"),\n",
    "            (F.sum(F.when(F.col(\"transaction_status\") == \"completed\", F.col(\"cdc_multiplier\")).otherwise(0))).alias(\"successful_transactions\"),\n",
    "            (F.sum(F.when(F.col(\"transaction_status\") == \"failed\", F.col(\"cdc_multiplier\")).otherwise(0))).alias(\"failed_transactions\"),\n",
    "            (F.sum(F.when(F.col(\"transaction_status\") == \"refunded\", F.col(\"cdc_multiplier\")).otherwise(0))).alias(\"refunded_transactions\"),\n",
    "            \n",
    "            (F.sum(F.col(\"transaction_amount\") * F.col(\"cdc_multiplier\"))).alias(\"total_transaction_amount\"),\n",
    "            (F.sum(F.when(F.col(\"transaction_status\") == \"completed\", F.col(\"transaction_amount\") * F.col(\"cdc_multiplier\")).otherwise(0))).alias(\"successful_transaction_amount\"),\n",
    "            (F.sum(F.when(F.col(\"transaction_status\") == \"failed\", F.col(\"transaction_amount\") * F.col(\"cdc_multiplier\")).otherwise(0))).alias(\"failed_transaction_amount\"),\n",
    "            (F.sum(F.when(F.col(\"transaction_status\") == \"refunded\", F.col(\"transaction_amount\") * F.col(\"cdc_multiplier\")).otherwise(0))).alias(\"refunded_transaction_amount\"),\n",
    "            \n",
    "            (F.sum(F.coalesce(F.col(\"processing_fee\"), F.lit(0)) * F.col(\"cdc_multiplier\"))).alias(\"total_processing_fee\"),\n",
    "            (F.sum(F.coalesce(F.col(\"commission\"), F.lit(0)) * F.col(\"cdc_multiplier\"))).alias(\"total_commission\"),\n",
    "            \n",
    "            (F.countDistinct(F.when(F.col(\"cdc_multiplier\") == 1, F.col(\"interac_id\")))).alias(\"unique_customers\"),\n",
    "            \n",
    "            F.min(F.when(F.col(\"cdc_multiplier\") == 1, F.col(\"transaction_timestamp\"))).alias(\"first_transaction_timestamp\"),\n",
    "            F.max(F.when(F.col(\"cdc_multiplier\") == 1, F.col(\"transaction_timestamp\"))).alias(\"last_transaction_timestamp\")\n",
    "        )\n",
    "\n",
    "        # Calculate success rate and net amount\n",
    "        merchant_aggregations = merchant_aggregations.withColumn(\n",
    "            \"success_rate\",\n",
    "            F.when(F.col(\"total_transactions\") > 0, \n",
    "                   (F.col(\"successful_transactions\") / F.col(\"total_transactions\")) * 100)\n",
    "            .otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"net_transaction_amount\",\n",
    "            F.col(\"successful_transaction_amount\") - F.col(\"refunded_transaction_amount\")\n",
    "        ).withColumn(\n",
    "            \"created_at\", F.current_timestamp()\n",
    "        ).withColumn(\n",
    "            \"updated_at\", F.current_timestamp()\n",
    "        )\n",
    "        \n",
    "        # Use simple merge function\n",
    "        merge_to_delta_table(merchant_agg_table, merchant_aggregations)\n",
    "        \n",
    "        records_processed = merchant_aggregations.count()\n",
    "        print(f\"Merchant aggregations processed: {records_processed} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing merchant aggregations: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "print(\"Merchant aggregation processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ee8e1f-f3c1-47d3-b85c-224a6be18707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming pipeline function defined\n"
     ]
    }
   ],
   "source": [
    "# Simple streaming query setup\n",
    "def start_streaming_pipeline():\n",
    "    \"\"\"Start the simple streaming pipeline\"\"\"\n",
    "    try:\n",
    "        print(\"Starting simple streaming pipeline...\")\n",
    "        \n",
    "        # Read streaming data from raw table using CDC\n",
    "        streaming_df = spark.readStream \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"readChangeFeed\", \"true\") \\\n",
    "            .option(\"startingVersion\", \"latest\") \\\n",
    "            .table(raw_table)\n",
    "        \n",
    "        # Process each batch\n",
    "        def process_batch(batch_df, batch_id):\n",
    "            print(f\"Processing batch {batch_id}\")\n",
    "            process_merchant_aggregations(batch_df, batch_id)\n",
    "            print(f\"Batch {batch_id} processed successfully.\")\n",
    "        \n",
    "        # Start streaming query\n",
    "        query = streaming_df.writeStream \\\n",
    "            .foreachBatch(process_batch) \\\n",
    "            .trigger(processingTime='30 seconds') \\\n",
    "            .option(\"checkpointLocation\", f\"/tmp/checkpoint_test_{catalog_name}_{schema_name}_merchant_agg\") \\\n",
    "            .start()\n",
    "        \n",
    "        print(\"Streaming pipeline started successfully!\")\n",
    "        return query\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error starting streaming pipeline: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "print(\"Streaming pipeline function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a0f7704-c755-4ed8-8a89-bd9594c69cea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simple streaming pipeline...\n",
      "Starting simple streaming pipeline...\n",
      "Streaming pipeline started successfully!\n",
      "Streaming pipeline is running!\n"
     ]
    }
   ],
   "source": [
    "# Start the streaming pipeline\n",
    "print(\"Starting simple streaming pipeline...\")\n",
    "streaming_query = start_streaming_pipeline()\n",
    "print(\"Streaming pipeline is running!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_realtime_streaming_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
